
# Projects description

#### 1. Parzen from scratch
Implement [Parzen](https://en.wikipedia.org/wiki/Kernel_density_estimation) with hard and soft windows, from scratch.
This project was part of a homework, with both theoretical and practical questions.

<br/>

#### 2. SVM from scratch
Implement [Support-Vector Machines](https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496) from scratch.
This project was part of a homework, with both theoretical and practical questions.

<br/>

#### 3. Fastforward neural network from scratch
Implement [Fastforward Neural Networks (FNNs)](http://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L1.pdf) from scratch.
This project was part of a homework, with both theoretical and practical questions.

<br/>

#### 4. Reddit comments classification *([Kaggle project](https://www.kaggle.com/c/ift3395-ift6390-reddit-comments/overview))*
Classify Reddit comments with their corresponding topics, amongst 20 classes going from "NBA" to "Music", etc.
The dataset was composed of 100.000 labeled comments: 70.000 for the training set and 30.000 for the test set, with lots of variations within the data like the comments' size, spelling mistakes, abbreviations, ...
The project was declined into two milestones: the first part was to build a Naïve Bayes model without any external library. For the second part we could build any desired model, with any libraries, and try to obtain the best accuracy possible.
We tried different approaches to this problem and achieved an accuracy of **55,5%** with our customed Naïve Bayes built from scratch, and **59,8%** by combining 2 classifiers: our Naïve Bayes and SVM, ranking 21st among 103 participants.

Our study is showcased in the joined *report_fr.pdf*, written in french (sorry).

*(`main.py` is the script of our first milestone: our customed Naïve Bayes)*

<br/>
<br/>

## Reference
© Ioannis Mitliagkas
University of Montreal, Canada
IFT 6390: Foundations of Machine Learning

